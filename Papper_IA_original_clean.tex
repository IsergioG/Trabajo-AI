\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{hyperref}
\setstretch{1.15}
\sloppy

\title{Proyecto: Extracción automática de información en facturas mediante IA}
\author{
	Universidad Jorge Tadeo Lozano \\ 
	Curso: Inteligencia Artificial -- Bogotá D.C., Colombia \\
	\vspace{0.5em}
	\textbf{Participantes:} \\ 
	Sergio Andrés Ramírez \\ 
Nelson David Rincon Osorio \\ 
Brayan Sebastian Garcia Cespedes
}
\date{Octubre 2025}

\begin{document}
	\maketitle
	
	\section{Resumen}
	Proponemos un sistema de extracción automática de información en facturas usando visión computacional y OCR sobre el dataset \textit{High-Quality Invoice Images for OCR} (Kaggle, +1 000 imágenes). El modelo combinará CNNs para segmentación y detección de texto con LayoutLM/LLM (ChatGPT o Llama) para la clasificación semántica de campos como NIT, fecha y valor total. Se espera alcanzar un F1 > 0.85 y una CER inferior a la de Tesseract, entregando un prototipo local funcional en 6 semanas. El desarrollo garantizará privacidad mediante anonimización y mitigará sesgos con datos diversos de facturas.
	
	\section{Problema local y motivación}
	En muchas empresas colombianas, los equipos contables deben asignar personal exclusivamente para el proceso de legalización y registro de facturas, lo que implica revisar y transcribir manualmente información como fechas, NIT, valores, conceptos y proveedores. Este procedimiento, además de ser repetitivo y propenso a errores humanos, consume una cantidad significativa de tiempo y recursos.
	
	Estas problemáticas motivan el desarrollo de un sistema automatizado de extracción y validación de datos en facturas, que permita agilizar la legalización documental, reducir errores de digitación y optimizar la carga laboral del personal contable. De esta forma, los profesionales del área podrán concentrar sus esfuerzos en tareas de análisis, control y supervisión, que realmente aporten valor a la gestión financiera de la organización.
	
	\section{Dataset}
	\textbf{Fuente y enlace; tamaño; variables; condiciones de uso/licencia; por qué es representativo.}
	
	\textbf{Enlace:} \\
	\url{https://www.kaggle.com/datasets/osamahosamabdellatif/high-quality-invoice-images-for-ocr/suggestions}
	
	La base de datos es representativa dado que tiene una amplia cantidad de facturas escaneadas, lo cual nos permite realizar el modelamiento y entrenamiento del programa que se busca construir.
	
	\section{Tarea de IA y algoritmo(s)}
	La tarea principal de IA en este proyecto es la “extracción automática de información” a partir de facturas escaneadas utilizando imágenes, es decir, una tarea de visión computacional combinada con procesamiento OCR (Reconocimiento Óptico de Caracteres). El sistema busca extraer campos clave como número de factura, fechas, montos y nombres de proveedores directamente de las imágenes de facturas digitales, como las proporcionadas por el dataset de Kaggle “High-Quality Invoice Images for OCR”.
	
	\subsection*{Tipo de tarea}
	\begin{itemize}
		\item Es una tarea multimodal, porque requiere tanto el análisis de imágenes (visión) como el procesamiento y estructuración del texto extraído (texto).
		\item El objetivo es aplicar OCR para convertir imágenes de facturas en texto digital y luego clasificar, localizar y extraer los diferentes campos relevantes.
	\end{itemize}
	
	\subsection*{Algoritmos y modelos propuestos}
	\begin{enumerate}
		\item \textbf{Modelos de Visión para OCR:}
		\begin{itemize}
			\item Un enfoque principal es entrenar y/o hacer \textit{fine-tuning} de una CNN (Red Neuronal Convolucional) sobre las imágenes del dataset para mejorar la detección y segmentación de las áreas relevantes de las facturas (cabeceras, totales, tablas de productos, etc.).
			\item Para la extracción de texto, se utilizarán modelos como Tesseract OCR o alternativas modernas basadas en \textit{deep learning} (por ejemplo, CRNN: Convolutional Recurrent Neural Network o Transformers como LayoutLMv3, diseñados para captar tanto texto como la disposición y relaciones espaciales en documentos).
		\end{itemize}
		
		\item \textbf{Modelos para análisis semántico:}
		\begin{itemize}
			\item Luego de obtener el texto, se pueden aplicar algoritmos NLP (Procesamiento de Lenguaje Natural) para clasificar los campos, validar la coherencia e identificar errores o valores atípicos, usando modelos tipo Llama o ChatGPT preentrenados.
		\end{itemize}
		
		\item \textbf{Justificación técnica:}
		\begin{itemize}
			\item Las CNNs se escogen por su rendimiento comprobado en tareas de segmentación y detección en imágenes.
			\item Los modelos OCR mencionados son estándar de la industria y están optimizados para facturas y documentos escaneados.
			\item LayoutLM y variantes de Transformers combinan visión y texto, aprovechando la disposición espacial del documento para mejorar la extracción por encima del OCR tradicional.
			\item El análisis posterior con modelos NLP permite categorizar, filtrar y validar los datos extraídos, aumentando precisión y utilidad.
		\end{itemize}
	\end{enumerate}
	
	\section{Metodología y evaluación}
	Para el desarrollo del proyecto, se seguirá una metodología estructurada que abarca desde el tratamiento inicial de los datos hasta la evaluación final del sistema integrado.
	
	\textbf{Preprocesamiento de datos:}
	\begin{itemize}
		\item \textbf{Imágenes (Facturas):} Las imágenes serán sometidas a un pipeline de preprocesamiento para optimizar la extracción de texto (binarización, eliminación de ruido, corrección de inclinación y normalización).
		\item \textbf{Texto Extraído:} Una vez que el modelo OCR extraiga el texto, se aplicarán técnicas de limpieza para normalizar los datos y estructurarlos para el análisis posterior.
	\end{itemize}
	
	\textbf{División de Datos:}
	\begin{itemize}
		\item 70\% para entrenamiento.
		\item 15\% para validación.
		\item 15\% para pruebas.
	\end{itemize}
	
	\textbf{Métricas de Evaluación:}
	\begin{itemize}
		\item \textbf{OCR:} CER (Tasa de Error de Caracteres) y WER (Tasa de Error de Palabras).
		\item \textbf{Clasificación:} Precisión, Recall y F1-Score. Objetivo: F1 > 0.85.
	\end{itemize}
	
	\textbf{Líneas Base y Comparación:} Se establecerá una línea base con Tesseract OCR y se comparará con el rendimiento del modelo CNN. El LLM se contrastará con un enfoque basado en expresiones regulares.
	
	\section{Resultados esperados, ética y cronograma}
	
	\textbf{Resultados esperados e hipótesis:}
	\begin{itemize}
		\item \textbf{Hipótesis 1 (Precisión superior):} La CNN logrará una CER menor que Tesseract.
		\item \textbf{Hipótesis 2 (Clasificación efectiva):} El sistema CNN + LLM alcanzará F1 > 0.85.
		\item \textbf{Hipótesis 3 (Prototipo funcional):} Se entregará un prototipo local capaz de procesar una imagen de factura y generar un documento estructurado.
	\end{itemize}
	
	\textbf{Consideraciones éticas y mitigación de riesgos:}
	\begin{itemize}
		\item \textbf{Privacidad de Datos:} Las facturas contienen información sensible.
		\begin{itemize}
			\item \textbf{Mitigación:} Se trabajará con datos anonimizados y entornos seguros. Se aplicará desenfoque o recorte de información personal.
		\end{itemize}
		\item \textbf{Sesgos en el Modelo:} El modelo podría tener bajo rendimiento en formatos no comunes.
		\begin{itemize}
			\item \textbf{Mitigación:} Garantizar diversidad de datos y análisis periódicos para detectar sesgos.
		\end{itemize}
		\item \textbf{Fiabilidad y Errores:} Un error en la extracción puede afectar resultados contables.
		\begin{itemize}
			\item \textbf{Mitigación:} Se incluirá una puntuación de confianza y validación cruzada para campos críticos.
		\end{itemize}
	\end{itemize}
	
	\textbf{Cronograma (resumen):}
	\begin{itemize}
		\item Semana 1: Definición y adquisición del dataset.
		\item Semana 2: Configuración del entorno y preprocesamiento.
		\item Semana 3: Entrenamiento inicial del modelo CNN.
		\item Semana 4: Integración OCR + LLM y ajuste fino.
		\item Semana 5: Evaluación cuantitativa.
		\item Semana 6: Documentación y entrega del prototipo.
	\end{itemize}
	
	\bibliographystyle{unsrt}
	\bibliography{referencias}
	
	
\end{document}
